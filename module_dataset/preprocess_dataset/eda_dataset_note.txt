##### eda about number sent and tokenizer in test submit dataset
number sentence of test base on sent tokenizer
{2: 841, 1: 914, 3: 235, 4: 31, 5: 5, 9: 1}

length of test base on tokenizer

{84: 33, 61: 61, 78: 35, 93: 16, 73: 34, 57: 36, 83: 30, 25: 40, 66: 52, 117: 5, 63: 27, 26: 38, 21: 31, 60: 48, 69: 49, 77: 33, 53: 41, 65: 35, 99: 22, 70: 51, 51: 25, 34: 27, 98: 9, 90: 14, 72: 43, 82: 38, 64: 46, 44: 36, 88: 23, 43: 48, 42: 34, 68: 59, 71: 36, 41: 27, 76: 19, 85: 25, 37: 28, 40: 19, 75: 42, 48: 25, 27: 44, 67: 49, 109: 16, 79: 21, 47: 27, 38: 16, 22: 19, 35: 31, 96: 15, 62: 44, 58: 67, 55: 41, 86: 16, 89: 16, 49: 21, 24: 41, 33: 24, 46: 28, 102: 8, 31: 32, 54: 49, 50: 26, 56: 24, 74: 33, 52: 32, 28: 52, 118: 4, 113: 1, 81: 29, 106: 11, 59: 39, 126: 3, 100: 15, 45: 25, 87: 18, 107: 11, 121: 6, 95: 13, 36: 24, 30: 45, 20: 16, 29: 28, 80: 20, 130: 3, 23: 21, 19: 6, 112: 5, 116: 1, 91: 19, 32: 25, 101: 8, 94: 17, 39: 30, 103: 6, 133: 1, 108: 7, 119: 8, 97: 5, 150: 1, 92: 20, 105: 7, 115: 3, 18: 3, 104: 1, 124: 2, 122: 2, 120: 1, 123: 2, 237: 1, 110: 5, 157: 1, 114: 2, 165: 1, 137: 1, 147: 2, 132: 3, 111: 2, 127: 1, 182: 1, 184: 2, 143: 2, 166: 2, 134: 2, 141: 2, 155: 1, 217: 1, 136: 1, 156: 1})


##### eda about number sent and tokenizer in train dataset
 defaultdict(<function check_length_test.<locals>.<lambda> at 0x7fbaa6e42bf8>, {1: 6332, 2: 4960, 4: 1098, 3: 2512, 5: 522, 7: 111, 6: 269, 8: 53, 16: 2, 10: 22, 9: 33, 11: 14, 12: 8, 18: 1, 19: 1, 15: 4, 14: 3, 13: 7, 27: 1, 17: 2, 20: 1, 23: 1})

len token bert of both query and document: 
 defaultdict(<function check_length_test.<locals>.<lambda> at 0x7fbaa6e42d90>, {22: 176, 99: 82, 95: 91, 70: 177, 141: 24, 97: 111, 53: 201, 25: 208, 26: 177, 71: 180, 128: 41, 46: 189, 66: 194, 81: 138, 56: 210, 40: 191, 31: 199, 103: 86, 158: 12, 88: 117, 51: 183, 37: 176, 167: 13, 52: 241, 65: 187, 74: 197, 133: 31, 190: 12, 48: 163, 30: 212, 86: 120, 42: 191, 55: 179, 20: 150, 486: 1, 149: 26, 45: 203, 12: 67, 35: 187, 27: 189, 193: 8, 87: 145, 38: 187, 114: 47, 135: 30, 13: 69, 61: 206, 67: 211, 94: 102, 44: 166, 79: 139, 43: 189, 68: 214, 243: 3, 36: 201, 23: 175, 60: 195, 117: 48, 41: 183, 76: 180, 49: 203, 92: 111, 64: 193, 91: 104, 62: 199, 16: 60, 115: 43, 32: 202, 21: 136, 313: 1, 208: 6, 33: 214, 105: 61, 85: 128, 10: 34, 69: 204, 28: 201, 58: 195, 72: 207, 221: 3, 73: 150, 50: 203, 82: 159, 19: 148, 147: 39, 89: 124, 112: 67, 24: 178, 154: 25, 47: 184, 3: 14, 168: 17, 98: 103, 80: 151, 29: 201, 219: 9, 155: 15, 150: 32, 188: 17, 111: 64, 138: 30, 125: 45, 59: 215, 14: 71, 78: 171, 110: 59, 54: 204, 34: 190, 63: 205, 170: 16, 151: 27, 39: 165, 122: 46, 7: 20, 84: 142, 4: 15, 131: 31, 15: 71, 223: 7, 96: 94, 126: 34, 211: 8, 75: 184, 176: 19, 165: 25, 77: 155, 83: 158, 8: 34, 123: 49, 57: 197, 9: 42, 333: 1, 140: 36, 17: 78, 237: 7, 196: 7, 137: 32, 113: 56, 121: 51, 100: 81, 161: 17, 241: 8, 108: 69, 174: 20, 134: 40, 129: 40, 202: 3, 136: 30, 106: 69, 124: 31, 179: 16, 109: 73, 159: 13, 189: 8, 127: 40, 130: 40, 262: 4, 119: 53, 132: 32, 120: 59, 107: 65, 192: 18, 142: 26, 146: 27, 139: 29, 181: 16, 296: 1, 18: 94, 143: 22, 93: 94, 198: 9, 187: 8, 304: 3, 101: 82, 229: 3, 325: 2, 116: 61, 164: 27, 118: 46, 209: 10, 145: 21, 186: 9, 285: 1, 276: 5, 90: 96, 200: 5, 157: 27, 426: 1, 242: 4, 171: 16, 228: 2, 166: 17, 104: 66, 163: 15, 156: 13, 269: 1, 298: 2, 173: 17, 144: 32, 184: 11, 194: 15, 11: 49, 6: 14, 152: 21, 102: 80, 162: 18, 197: 5, 183: 6, 286: 3, 195: 10, 247: 5, 306: 1, 258: 3, 180: 11, 251: 2, 205: 4, 222: 4, 204: 4, 212: 6, 178: 25, 240: 4, 234: 7, 255: 9, 245: 3, 231: 5, 177: 13, 153: 14, 501: 1, 218: 5, 355: 2, 273: 3, 203: 12, 279: 2, 172: 13, 305: 1, 232: 5, 5: 16, 191: 6, 207: 9, 224: 12, 307: 1, 148: 20, 235: 2, 210: 3, 175: 8, 514: 1, 265: 1, 201: 10, 182: 12, 267: 2, 185: 8, 256: 4, 271: 6, 311: 3, 332: 1, 387: 1, 314: 1, 280: 2, 239: 3, 342: 2, 259: 3, 351: 1, 297: 2, 336: 1, 277: 2, 282: 4, 254: 4, 199: 6, 220: 6, 244: 5, 344: 1, 169: 6, 214: 6, 215: 5, 281: 4, 398: 1, 317: 1, 230: 8, 260: 3, 405: 1, 472: 1, 301: 1, 160: 14, 249: 7, 309: 2, 252: 1, 352: 4, 233: 5, 360: 1, 434: 1, 310: 2, 250: 1, 330: 1, 2: 3, 380: 1, 318: 1, 358: 1, 270: 2, 213: 4, 289: 3, 274: 1, 236: 4, 489: 1, 217: 3, 370: 2, 453: 1, 263: 1, 278: 1, 288: 2, 439: 1, 382: 1, 299: 1, 253: 1, 272: 2, 553: 1, 266: 3, 264: 1, 551: 1, 343: 1, 364: 2, 216: 3, 326: 2, 390: 1, 442: 1, 225: 4, 388: 1, 492: 1, 227: 2, 226: 5, 526: 1, 412: 1, 656: 1, 238: 3, 206: 3, 292: 1, 354: 1, 417: 1, 322: 1, 549: 1, 261: 1, 294: 1, 246: 1, 356: 1, 308: 1, 328: 1, 400: 1, 345: 1, 312: 1, 349: 1, 452: 1, 545: 1, 327: 2, 248: 2})

## eda on squad
{5: 3276, 4: 3758, 3: 2806, 6: 2317, 2: 1487, 7: 1421, 1: 469, 8: 889, 11: 145, 10: 269, 9: 484, 12: 70, 14: 20, 13: 41, 19: 1, 15: 5, 22: 1, 17: 1})
len token bert of both query and document: 
  {201: 518, 279: 131, 166: 614, 217: 321, 352: 98, 371: 51, 200: 336, 301: 182, 228: 275, 309: 129, 305: 135, 176: 696, 224: 405, 300: 159, 189: 567, 114: 216, 148: 831, 249: 208, 83: 129, 152: 673, 283: 182, 46: 107, 206: 400, 172: 613, 239: 301, 180: 593, 104: 196, 63: 111, 284: 111, 153: 664, 178: 663, 220: 418, 197: 466, 242: 256, 170: 649, 171: 677, 164: 551, 159: 812, 156: 726, 98: 162, 128: 595, 165: 797, 348: 43, 341: 82, 302: 120, 111: 153, 177: 696, 234: 350, 219: 465, 243: 281, 82: 133, 79: 145, 226: 316, 244: 261, 229: 327, 257: 264, 110: 143, 296: 143, 143: 782, 251: 252, 150: 794, 230: 339, 268: 242, 154: 685, 115: 276, 124: 371, 374: 46, 368: 72, 187: 609, 191: 675, 335: 83, 299: 123, 333: 89, 198: 412, 199: 438, 106: 124, 113: 207, 212: 390, 340: 57, 149: 747, 93: 151, 58: 100, 136: 584, 120: 373, 175: 667, 123: 464, 86: 132, 205: 451, 262: 165, 92: 141, 135: 553, 142: 716, 216: 400, 129: 549, 75: 124, 158: 788, 66: 81, 223: 386, 173: 622, 132: 725, 84: 92, 73: 97, 195: 546, 146: 737, 328: 114, 254: 278, 275: 180, 188: 614, 221: 380, 137: 734, 303: 111, 255: 246, 95: 135, 147: 703, 204: 528, 272: 230, 102: 139, 130: 456, 168: 723, 133: 601, 321: 91, 258: 248, 358: 37, 267: 238, 203: 382, 292: 66, 69: 113, 338: 69, 160: 771, 121: 264, 210: 427, 71: 150, 247: 256, 322: 73, 80: 132, 97: 177, 343: 68, 314: 78, 313: 97, 320: 142, 357: 27, 317: 124, 231: 280, 213: 433, 307: 76, 270: 216, 194: 506, 117: 213, 227: 333, 399: 50, 367: 55, 144: 802, 103: 163, 67: 94, 76: 133, 125: 427, 370: 61, 264: 202, 112: 180, 49: 101, 211: 443, 183: 505, 122: 285, 295: 90, 174: 693, 85: 126, 297: 207, 37: 45, 91: 153, 90: 113, 127: 562, 81: 176, 126: 411, 38: 30, 108: 158, 141: 760, 94: 138, 100: 113, 68: 127, 218: 432, 96: 136, 105: 98, 151: 882, 182: 598, 259: 267, 351: 100, 208: 368, 101: 153, 332: 122, 184: 547, 52: 98, 99: 171, 240: 235, 161: 731, 281: 237, 316: 109, 263: 150, 140: 626, 391: 34, 88: 128, 70: 134, 233: 346, 56: 115, 327: 127, 207: 341, 118: 271, 72: 102, 288: 117, 225: 341, 59: 97, 375: 51, 139: 720, 51: 59, 48: 74, 285: 161, 232: 292, 89: 164, 181: 578, 119: 365, 65: 102, 290: 164, 389: 21, 78: 101, 145: 756, 109: 142, 185: 550, 193: 449, 87: 122, 131: 702, 235: 216, 134: 710, 162: 726, 380: 33, 186: 513, 319: 106, 222: 332, 163: 711, 350: 99, 318: 119, 304: 122, 202: 467, 33: 20, 330: 115, 236: 302, 74: 87, 190: 611, 246: 333, 308: 67, 266: 137, 250: 275, 155: 779, 57: 100, 289: 156, 280: 142, 337: 102, 336: 55, 271: 180, 349: 57, 260: 211, 364: 36, 324: 95, 331: 59, 312: 129, 53: 97, 55: 80, 157: 565, 298: 200, 196: 527, 62: 104, 274: 153, 269: 189, 192: 533, 256: 210, 209: 415, 397: 27, 238: 297, 40: 46, 376: 32, 248: 281, 347: 76, 387: 19, 277: 175, 41: 57, 310: 119, 378: 65, 179: 601, 60: 117, 116: 235, 215: 281, 384: 45, 169: 682, 61: 90, 138: 679, 237: 269, 214: 438, 363: 40, 50: 95, 47: 67, 54: 58, 107: 118, 241: 332, 44: 50, 167: 545, 291: 98, 42: 60, 381: 23, 362: 94, 252: 242, 294: 118, 253: 177, 311: 98, 405: 34, 403: 8, 273: 197, 356: 61, 265: 176, 45: 59, 404: 36, 36: 6, 43: 37, 64: 104, 342: 46, 329: 107, 278: 234, 77: 83, 30: 4, 407: 29, 245: 282, 398: 35, 334: 65, 315: 161, 390: 36, 35: 7, 393: 25, 325: 85, 360: 62, 392: 19, 39: 27, 286: 119, 306: 101, 372: 49, 282: 158, 345: 84, 261: 231, 287: 149, 326: 66, 394: 69, 395: 15, 379: 33, 293: 123, 34: 4, 373: 66, 276: 109, 377: 35, 427: 16, 409: 13, 365: 38, 359: 51, 344: 20, 422: 19, 355: 65, 414: 7, 385: 46, 388: 7, 383: 16, 323: 90, 361: 49, 346: 73, 440: 1, 402: 54, 354: 59, 411: 24, 366: 56, 415: 23, 429: 14, 382: 27, 386: 10, 410: 17, 339: 29, 400: 7, 401: 57, 396: 20, 412: 10, 29: 3, 460: 5, 421: 8, 481: 1, 406: 8, 413: 19, 416: 20, 353: 39, 369: 14, 448: 1, 418: 10, 457: 4, 408: 10, 426: 10, 430: 4, 423: 5, 428: 4})

python ./examples/run_squad.py --model_type bert --model_name_or_path bert-base-multilingual-uncased  --do_eval --do_lower_case --train_file cache_bert/train-v2.0.json --predict_file cache_bert/dev-v2.0.json --version_2_with_negative --max_seq_length 400 --doc_stride 128 --output_dir output_bert/ --per_gpu_eval_batch_size=8 --save_steps=400 --cache_dir cache_bert/ 

python -m torch.distributed.launch --nproc_per_node=2 ./examples/run_squad.py --model_type bert --model_name_or_path bert-base-multilingual-uncased --do_train --do_eval --do_lower_case --train_file dataset/train-v2.0.json --predict_file dataset/dev-v2.0.json --version_2_with_negative --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 430 --doc_stride 128 --output_dir output_bert/ --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=8 --gradient_accumulation_steps=3 --save_steps=400 --cache_dir cache_bert/ 

python -m torch.distributed.launch --nproc_per_node=2 ./examples/run_squad.py --model_type bert --model_name_or_path bert-base-multilingual-uncased --do_train --do_eval --do_lower_case --train_file dataset/train-v2.0.json --predict_file dataset/dev-v2.0.json --version_2_with_negative --learning_rate 3e-5 --num_train_epochs 3 --max_seq_length 430 --doc_stride 128 --output_dir output_bert/ --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=8 --gradient_accumulation_steps=3 --save_steps=200 --cache_dir cache_bert/

python ./examples/run_squad.py --model_type bert --model_name_or_path bert-base-multilingual-uncased  --do_eval --do_lower_case --eval_all_checkpoints --train_file dataset/train-v2.0.json --predict_file dataset/dev-v2.0.json --version_2_with_negative --max_seq_length 430 --doc_stride 128 --output_dir output_bert/ --per_gpu_eval_batch_size=8 --cache_dir cache_bert/ --n_best_size 1

run viet mai long
python ./examples/run_squad.py --model_type bert --model_name_or_path checkpoint-7000/ --do_train --do_eval --do_lower_case --evaluate_during_training --train_file dataset/train-v2.0-viet_mailong.json --predict_file dataset/dev-v2.0-viet_mailong.json --version_2_with_negative --learning_rate 2e-5 --num_train_epochs 2 --max_seq_length 400 --doc_stride 128 --output_dir output_bert/ --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=6 --gradient_accumulation_steps=4 --logging_steps 50 --save_steps=1000 --cache_dir cache_bert/ 